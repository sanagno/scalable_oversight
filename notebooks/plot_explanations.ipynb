{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.utils import load_pickle\n",
    "from src.dataset_utils import get_dataset\n",
    "from src.model_utils import get_model, get_model_generations\n",
    "from src.definitions import LEVELS\n",
    "from argparse import Namespace\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-18 16:35:01 config.py:193] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 03-18 16:35:01 llm_engine.py:87] Initializing an LLM engine with config: model='TheBloke/Llama-2-13B-chat-GPTQ', tokenizer='TheBloke/Llama-2-13B-chat-GPTQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-18 16:35:05 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 03-18 16:35:09 llm_engine.py:357] # GPU blocks: 1068, # CPU blocks: 327\n",
      "INFO 03-18 16:35:10 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-18 16:35:10 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-18 16:35:11 model_runner.py:756] Graph capturing finished in 1 secs.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model(\"Llama-2-13b-chat\", None, \"int8\", torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:47<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa assistant 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:48<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa level_1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:48<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa level_2 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:48<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa level_3 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:48<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa level_4 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:48<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa level_5 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 600/600 [00:52<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siqa assistant 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 600/600 [00:52<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siqa level_1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 600/600 [00:52<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siqa level_2 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 600/600 [00:52<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siqa level_3 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 600/600 [00:52<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siqa level_4 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 600/600 [00:52<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siqa level_5 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1000/1000 [01:29<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa assistant 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1000/1000 [01:29<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa level_1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1000/1000 [01:29<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa level_2 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1000/1000 [01:29<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa level_3 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1000/1000 [01:29<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa level_4 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1000/1000 [01:29<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa level_5 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:18<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa assistant 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:21<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa level_1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:18<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa level_2 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:18<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa level_3 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:18<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa level_4 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:18<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa level_5 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 570/570 [01:37<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_qa assistant 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 570/570 [01:37<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_qa level_1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 570/570 [01:37<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_qa level_2 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 570/570 [01:37<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_qa level_3 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 570/570 [01:37<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_qa level_4 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 570/570 [01:37<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_qa level_5 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 792/792 [02:17<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpqa assistant 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 792/792 [02:17<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpqa level_1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 792/792 [02:17<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpqa level_2 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 792/792 [02:17<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpqa level_3 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 792/792 [02:17<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpqa level_4 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 792/792 [02:17<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpqa level_5 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:42<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality assistant 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:42<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality level_1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:42<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality level_2 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:42<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality level_3 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:42<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality level_4 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [01:42<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality level_5 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:36<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolq assistant 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:36<00:00, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolq level_1 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:36<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolq level_2 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:36<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolq level_3 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:36<00:00, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolq level_4 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 400/400 [00:36<00:00, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolq level_5 1.0\n",
      "{'piqa_assistant': 1.0, 'piqa_level_1': 1.0, 'piqa_level_2': 1.0, 'piqa_level_3': 1.0, 'piqa_level_4': 1.0, 'piqa_level_5': 1.0, 'siqa_assistant': 1.0, 'siqa_level_1': 1.0, 'siqa_level_2': 1.0, 'siqa_level_3': 1.0, 'siqa_level_4': 1.0, 'siqa_level_5': 1.0, 'commonsense_qa_assistant': 1.0, 'commonsense_qa_level_1': 1.0, 'commonsense_qa_level_2': 1.0, 'commonsense_qa_level_3': 1.0, 'commonsense_qa_level_4': 1.0, 'commonsense_qa_level_5': 1.0, 'openbookqa_assistant': 1.0, 'openbookqa_level_1': 1.0, 'openbookqa_level_2': 1.0, 'openbookqa_level_3': 1.0, 'openbookqa_level_4': 1.0, 'openbookqa_level_5': 1.0, 'wiki_qa_assistant': 1.0, 'wiki_qa_level_1': 1.0, 'wiki_qa_level_2': 1.0, 'wiki_qa_level_3': 1.0, 'wiki_qa_level_4': 1.0, 'wiki_qa_level_5': 1.0, 'gpqa_assistant': 1.0, 'gpqa_level_1': 1.0, 'gpqa_level_2': 1.0, 'gpqa_level_3': 1.0, 'gpqa_level_4': 1.0, 'gpqa_level_5': 1.0, 'quality_assistant': 1.0, 'quality_level_1': 1.0, 'quality_level_2': 1.0, 'quality_level_3': 1.0, 'quality_level_4': 1.0, 'quality_level_5': 1.0, 'boolq_assistant': 1.0, 'boolq_level_1': 1.0, 'boolq_level_2': 1.0, 'boolq_level_3': 1.0, 'boolq_level_4': 1.0, 'boolq_level_5': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    \"piqa\",\n",
    "    \"siqa\",\n",
    "    \"commonsense_qa\",\n",
    "    \"openbookqa\",\n",
    "    \"wiki_qa\",\n",
    "    \"gpqa\",\n",
    "    \"quality\",\n",
    "    \"boolq\",\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    for level in LEVELS:\n",
    "        advocate_level = \"assistant\"\n",
    "\n",
    "        dataset = get_dataset(\n",
    "            args=Namespace(base_data_folder=\"./data\"),\n",
    "            dataset_name=dataset_name,\n",
    "            cache_dir=None,\n",
    "            advocate_level=\"None\",\n",
    "            return_raw=True,\n",
    "        )\n",
    "\n",
    "        advocate_dataset = load_pickle(\n",
    "            f\"data/advocate_data/{dataset_name}/Llama-2-70b-chat/generations_{advocate_level}.pkl\"\n",
    "        )\n",
    "\n",
    "        dataset_idx = 0\n",
    "        dataset_advocate_idx = 0\n",
    "\n",
    "        data = []\n",
    "\n",
    "        while dataset_advocate_idx < len(advocate_dataset):\n",
    "            answers = (\n",
    "                dataset[dataset_idx][\"correct_answers\"]\n",
    "                + dataset[dataset_idx][\"incorrect_answers\"]\n",
    "            )\n",
    "            for idx in range(len(answers)):\n",
    "                advocate_answer = answers[idx]\n",
    "                generation = advocate_dataset[dataset_advocate_idx][\"generation\"]\n",
    "                other_answers = [\n",
    "                    answer for answer in answers if answer != advocate_answer\n",
    "                ]\n",
    "\n",
    "                data.append(\n",
    "                    f\"Here is an explanation. ```{generation}```. Does this explanation promote the answer '{advocate_answer}' compared to the answers `{other_answers}`? Answer: Yes/No\"\n",
    "                )\n",
    "\n",
    "                dataset_advocate_idx += 1\n",
    "            dataset_idx += 1\n",
    "\n",
    "        conversation_data = [\n",
    "            {\n",
    "                \"conversation_history\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": x},\n",
    "                ]\n",
    "            }\n",
    "            for x in data\n",
    "        ]\n",
    "\n",
    "        generations = get_model_generations(\n",
    "            model, tokenizer, conversation_data, max_new_tokens=5, do_sample=False\n",
    "        )\n",
    "\n",
    "        res = np.mean(\n",
    "            [\n",
    "                True if \"yes\" in x[\"generation\"].lower() else False\n",
    "                for x in generations\n",
    "                if any([y in x[\"generation\"].lower() for y in [\"yes\", \"np\"]])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(dataset_name, level, res)\n",
    "        results[f\"{dataset_name}_{level}\"] = res\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
